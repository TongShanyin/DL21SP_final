Dataset is copied to /tmp
Start Training
[1,    10] loss: 15304425494357.072
[1,    20] loss: 6913554452480.000
[1,    30] loss: 1335572412006.400
[1,    40] loss: 347827337216.000
[1,    50] loss: 48063869081.600
[1,    60] loss: 30045869465.600
[1,    70] loss: 13298150041.600
[1,    80] loss: 1987555436.800
[1,    90] loss: 521863446.400
[1,   100] loss: 278005116.800
[1,   110] loss: 182040926.400
[1,   120] loss: 131218378.400
[1,   130] loss: 106272509.600
[1,   140] loss: 96284727.200
[1,   150] loss: 89460372.800
[1,   160] loss: 84919023.200
[1,   170] loss: 80027696.800
[1,   180] loss: 76953011.200
[1,   190] loss: 72417948.800
[1,   200] loss: 71339844.000
[1,   210] loss: 66913000.000
[1,   220] loss: 64003864.400
[1,   230] loss: 61638943.600
[1,   240] loss: 59062829.200
[1,   250] loss: 56768907.600
[1,   260] loss: 54775555.600
[1,   270] loss: 51939088.000
[1,   280] loss: 49624006.400
[1,   290] loss: 47045879.600
[1,   300] loss: 45015076.800
[1,   310] loss: 44331448.000
[1,   320] loss: 42579922.400
[1,   330] loss: 40087263.600
[1,   340] loss: 38852825.600
[1,   350] loss: 37397376.400
[1,   360] loss: 36425763.200
[1,   370] loss: 34677900.800
[1,   380] loss: 33451163.000
[1,   390] loss: 32284462.800
[1,   400] loss: 31071538.600
[1,   410] loss: 29902764.000
[1,   420] loss: 28538826.200
[1,   430] loss: 27329013.800
[1,   440] loss: 26948103.000
[1,   450] loss: 25634060.400
[1,   460] loss: 24900260.800
[1,   470] loss: 24183835.400
[1,   480] loss: 23097260.200
[1,   490] loss: 22420444.400
[1,   500] loss: 21755342.800
[1,   510] loss: 20929810.200
[1,   520] loss: 20717534.400
[1,   530] loss: 19993054.400
[1,   540] loss: 19098684.000
[1,   550] loss: 18331082.200
[1,   560] loss: 18278616.800
[1,   570] loss: 17618416.100
[1,   580] loss: 17102377.900
[1,   590] loss: 16431150.200
[1,   600] loss: 15840484.500
[1,   610] loss: 15562544.700
[1,   620] loss: 14864253.300
[1,   630] loss: 14397142.700
[1,   640] loss: 13968315.600
[1,   650] loss: 13823324.800
[1,   660] loss: 13373129.100
[1,   670] loss: 13085162.400
[1,   680] loss: 12651675.900
[1,   690] loss: 12213108.900
[1,   700] loss: 11918016.300
[1,   710] loss: 11509465.300
[1,   720] loss: 11340185.000
[1,   730] loss: 11089814.200
[1,   740] loss: 10623997.800
[1,   750] loss: 10463103.700
[1,   760] loss: 10183667.300
[1,   770] loss: 9938452.600
[1,   780] loss: 9771422.300
[1,   790] loss: 9462688.500
[1,   800] loss: 9214319.100
[1,   810] loss: 9081777.300
[1,   820] loss: 8735778.150
[1,   830] loss: 8624604.800
[1,   840] loss: 8387391.050
[1,   850] loss: 8227671.250
[1,   860] loss: 8038465.700
[1,   870] loss: 7860440.000
[1,   880] loss: 7599592.600
[1,   890] loss: 7589761.150
[1,   900] loss: 7399486.250
[1,   910] loss: 7230115.650
[1,   920] loss: 7143069.650
[1,   930] loss: 6880621.200
[1,   940] loss: 6732818.500
[1,   950] loss: 6806707.650
[1,   960] loss: 6581049.600
[1,   970] loss: 6469468.850
[1,   980] loss: 6345327.350
[1,   990] loss: 6219507.250
[1,  1000] loss: 6097531.900
[1,  1010] loss: 5885965.900
[1,  1020] loss: 5848762.150
[1,  1030] loss: 5840945.050
[1,  1040] loss: 5702505.150
[1,  1050] loss: 5490271.400
[1,  1060] loss: 5524971.950
[1,  1070] loss: 5378548.800
[1,  1080] loss: 5308283.550
[1,  1090] loss: 5219276.400
[1,  1100] loss: 5026154.650
[1,  1110] loss: 5020059.100
[1,  1120] loss: 4953250.250
[1,  1130] loss: 4916441.150
[1,  1140] loss: 4838271.250
[1,  1150] loss: 4774799.350
[1,  1160] loss: 4633781.850
[1,  1170] loss: 4628152.350
[1,  1180] loss: 4539703.200
[1,  1190] loss: 4494745.100
[1,  1200] loss: 4428571.700
[1,  1210] loss: 4334103.025
[1,  1220] loss: 4266476.175
[1,  1230] loss: 4217001.175
[1,  1240] loss: 4201860.150
[1,  1250] loss: 4058377.625
[1,  1260] loss: 4025384.800
[1,  1270] loss: 3991973.925
[1,  1280] loss: 3926028.125
[1,  1290] loss: 3936665.700
[1,  1300] loss: 3793226.175
[1,  1310] loss: 3859696.025
[1,  1320] loss: 3756134.100
[1,  1330] loss: 3740341.700
[1,  1340] loss: 3703913.275
[1,  1350] loss: 3612210.050
[1,  1360] loss: 3542788.675
[1,  1370] loss: 3588708.350
[1,  1380] loss: 3531117.225
[1,  1390] loss: 3489421.100
[1,  1400] loss: 3457834.850
[1,  1410] loss: 3362615.850
[1,  1420] loss: 3356462.275
[1,  1430] loss: 3379004.725
[1,  1440] loss: 3299676.275
[1,  1450] loss: 3229880.025
[1,  1460] loss: 3246289.350
[1,  1470] loss: 3167097.225
[1,  1480] loss: 3195641.650
[1,  1490] loss: 3078884.375
[1,  1500] loss: 3064671.275
[1,  1510] loss: 3048450.600
[1,  1520] loss: 3060292.800
[1,  1530] loss: 2955738.925
[1,  1540] loss: 2967660.000
[1,  1550] loss: 3014638.625
[1,  1560] loss: 2864629.375
[1,  1570] loss: 2879316.150
[1,  1580] loss: 2826065.925
[1,  1590] loss: 2823805.025
[1,  1600] loss: 2782344.025
[1,  1610] loss: 2789711.625
[1,  1620] loss: 2730074.200
[1,  1630] loss: 2702154.675
[1,  1640] loss: 2671091.325
[1,  1650] loss: 2668169.600
[1,  1660] loss: 2638853.500
[1,  1670] loss: 2605734.450
[1,  1680] loss: 2631383.650
[1,  1690] loss: 2577358.375
[1,  1700] loss: 2571444.450
[1,  1710] loss: 2539071.975
[1,  1720] loss: 2501059.875
[1,  1730] loss: 2481990.050
[1,  1740] loss: 2467085.250
[1,  1750] loss: 2451828.850
[1,  1760] loss: 2429693.900
[1,  1770] loss: 2425268.625
[1,  1780] loss: 2377437.975
[1,  1790] loss: 2348040.150
[1,  1800] loss: 2372398.125
[1,  1810] loss: 2302153.050
[1,  1820] loss: 2289955.700
[1,  1830] loss: 2268894.225
[1,  1840] loss: 2274389.900
[1,  1850] loss: 2254016.825
[1,  1860] loss: 2206959.075
[1,  1870] loss: 2218510.450
[1,  1880] loss: 2167382.300
[1,  1890] loss: 2179020.125
[1,  1900] loss: 2154899.837
[1,  1910] loss: 2133102.050
[1,  1920] loss: 2099426.825
[1,  1930] loss: 2100337.575
[1,  1940] loss: 2067940.637
[1,  1950] loss: 2065656.288
[1,  1960] loss: 2041375.462
[1,  1970] loss: 2026535.712
[1,  1980] loss: 1994108.812
[1,  1990] loss: 1985215.850
[1,  2000] loss: 1978457.488
Finished Training
Saved checkpoint to checkpoints/encoder_big.pth
